# app.py
import streamlit as st
import ui                   # UIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import llm                  # LLMãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import database             # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import metrics              # è©•ä¾¡æŒ‡æ¨™ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import data                 # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import torch
from transformers import pipeline
from config import MODEL_NAME
from huggingface_hub import HfFolder
import os
import time

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š ---
st.set_page_config(page_title="Gemma Chatbot", layout="wide")

# --- åˆæœŸåŒ–å‡¦ç† ---
# NLTKãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›èµ·å‹•æ™‚ãªã©ï¼‰
metrics.initialize_nltk()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ä½œæˆï¼‰
database.init_db()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒç©ºãªã‚‰ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥
data.ensure_initial_data()

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–¢æ•°
def log_performance(action, start_time):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹"""
    elapsed = time.time() - start_time
    st.session_state.perf_logs.append(f"{action}: {elapsed:.2f}ç§’")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'perf_logs' not in st.session_state:
    st.session_state.perf_logs = []

# LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ï¼‰
@st.cache_resource
def load_model():
    """LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    start_time = time.time()
    try:
        # GPUãƒ¡ãƒ¢ãƒªã®ä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèªï¼ˆCUDAãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if torch.cuda.is_available():
            device = "cuda"
            mem_info = torch.cuda.mem_get_info()
            free_mem = mem_info[0] / (1024 ** 3)  # GBã«å¤‰æ›
            total_mem = mem_info[1] / (1024 ** 3)  # GBã«å¤‰æ›
            st.info(f"GPU: {torch.cuda.get_device_name(0)} - ç©ºããƒ¡ãƒ¢ãƒª: {free_mem:.2f}GB / åˆè¨ˆ: {total_mem:.2f}GB")
        else:
            device = "cpu"
            st.info(f"Using device: {device} (GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“)")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å‡¦ç†
        pipe = pipeline(
            "text-generation",
            model=MODEL_NAME,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device=device
        )
        log_performance("ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰", start_time)
        st.success(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚(æ‰€è¦æ™‚é–“: {time.time() - start_time:.2f}ç§’)")
        return pipe
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.error("GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ä¸è¦ãªãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã™ã‚‹ã‹ã€ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        return None

# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
pipe = load_model()

# --- Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
st.title("ğŸ¤– Gemma 2 Chatbot with Feedback")
st.write("Gemmaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚å›ç­”ã«å¯¾ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¡Œãˆã¾ã™ã€‚")
st.markdown("---")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
st.sidebar.title("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ä½¿ç”¨ã—ã¦é¸æŠãƒšãƒ¼ã‚¸ã‚’ä¿æŒ
if 'page' not in st.session_state:
    st.session_state.page = "ãƒãƒ£ãƒƒãƒˆ" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒšãƒ¼ã‚¸

page = st.sidebar.radio(
    "ãƒšãƒ¼ã‚¸é¸æŠ",
    ["ãƒãƒ£ãƒƒãƒˆ", "å±¥æ­´é–²è¦§", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†", "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹"],
    key="page_selector",
    index=["ãƒãƒ£ãƒƒãƒˆ", "å±¥æ­´é–²è¦§", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†", "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹"].index(st.session_state.page)
    if st.session_state.page in ["ãƒãƒ£ãƒƒãƒˆ", "å±¥æ­´é–²è¦§", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†", "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹"] else 0,
    on_change=lambda: setattr(st.session_state, 'page', st.session_state.page_selector)
)

# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
if st.session_state.page == "ãƒãƒ£ãƒƒãƒˆ":
    if pipe:
        ui.display_chat_page(pipe)
    else:
        st.error("ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
elif st.session_state.page == "å±¥æ­´é–²è¦§":
    ui.display_history_page()
elif st.session_state.page == "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†":
    ui.display_data_page()
elif st.session_state.page == "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹":
    st.header("ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º
    st.subheader("ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Python Version", os.sys.version.split()[0])
        st.metric("Torch Version", torch.__version__)
    with col2:
        st.metric("CUDA Available", "Yes" if torch.cuda.is_available() else "No")
        if torch.cuda.is_available():
            st.metric("CUDA Version", torch.version.cuda)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°ã®è¡¨ç¤º
    st.subheader("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°")
    if st.session_state.perf_logs:
        for log in st.session_state.perf_logs:
            st.text(log)
    else:
        st.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯ã¾ã è¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    
    if st.button("ãƒ­ã‚°ã‚’ã‚¯ãƒªã‚¢"):
        st.session_state.perf_logs = []
        st.experimental_rerun()

# --- ãƒ•ãƒƒã‚¿ãƒ¼ãªã©ï¼ˆä»»æ„ï¼‰ ---
st.sidebar.markdown("---")
st.sidebar.info("é–‹ç™ºè€…: [Your Name]")
st.sidebar.text(f"æœ€çµ‚æ›´æ–°: {time.strftime('%Y-%m-%d %H:%M')}")
